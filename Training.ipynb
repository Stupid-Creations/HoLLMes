{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "mount_file_id": "1C7jh9r18ygTsmi3A01m90w85juAQZifr",
      "authorship_tag": "ABX9TyODCOHo/1LgOa1uqCWSEI8S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stupid-Creations/HoLLMes/blob/main/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "\n",
        "embed_size = 384\n",
        "block_size = 256\n",
        "dropout = 0.2\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#print(device)"
      ],
      "metadata": {
        "id": "3-kgmAjAJepK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(\"drive/MyDrive/YAY.txt\",\"r\").read()\n",
        "preprocessed = re.split('([\\s])',text)\n",
        "\n",
        "vocab = sorted(list(set(preprocessed)))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "encode = lambda x: [vocab.index(i) for i in x]\n",
        "decode = lambda x: ''.join([vocab[i] for i in x])\n",
        "\n",
        "print(vocab)\n",
        "\n",
        "print(decode(encode(preprocessed[:100])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6_-GHwVK68y",
        "outputId": "193324d9-3d50-419d-9e79-7c9793427385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREFACE\n",
            "\n",
            "I fear that Mr. Sherlock Holmes may become like one of those popular\n",
            "tenors who, having outlived their time, are still tempted to make\n",
            "repeated farewell bows to their indulgent audiences.  This must cease\n",
            "and he must go the way of all flesh, material or imaginary.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(encode(re.split('([\\s])',\"He Creeping Cunard\"))))\n",
        "tokenized = torch.tensor(encode(preprocessed))\n",
        "train_data = tokenized[:int(len(tokenized)*0.9)]\n",
        "val_data = tokenized[int(len(tokenized)*0.9):]\n",
        "print(len(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftbMUKcvObTo",
        "outputId": "f7f51785-f210-4e1f-b31c-386d8ed20e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He Creeping Cunard\n",
            "161072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (32,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "xb,yb = get_batch('train')"
      ],
      "metadata": {
        "id": "dA2zc3KEQ5Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class trans_block(nn.Module):\n",
        "    def __init__(self,embed_size,heads):\n",
        "        super().__init__()\n",
        "        head_size = embed_size // heads\n",
        "        self.attention = Heads(heads,head_size)\n",
        "        self.ff_layer = FF_Layer(embed_size)\n",
        "        self.lnorm1 = nn.LayerNorm(embed_size)\n",
        "        self.lnorm2 = nn.LayerNorm(embed_size)\n",
        "    def forward(self,x):\n",
        "        x = x + self.attention(self.lnorm1(x))\n",
        "        x = x + self.ff_layer(self.lnorm2(x))\n",
        "        return x\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self,headsize):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(embed_size,headsize,bias=False)\n",
        "        self.query = nn.Linear(embed_size,headsize,bias=False)\n",
        "        self.value = nn.Linear(embed_size,headsize,bias=False)\n",
        "        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        Batches, Time, Channels = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2,-1) * Channels**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:Time,:Time] == 0,float('-inf'))\n",
        "        wei = F.softmax(wei,dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class Heads(nn.Module):\n",
        "    def __init__(self,n_head,head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for i in range(n_head)])\n",
        "        self.projection = nn.Linear(embed_size, embed_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        out = torch.cat([head(x) for head in self.heads],dim=-1)\n",
        "        out = self.dropout(self.projection(out))\n",
        "        return out\n",
        "\n",
        "class FF_Layer(nn.Module):\n",
        "    def __init__(self,embed_size):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_size,4*embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*embed_size,embed_size),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.net(x)\n",
        "class BigramLM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding_table = nn.Embedding(vocab_size,embed_size)\n",
        "        self.position_embedding_table = nn.Embedding(block_size,embed_size)\n",
        "        self.lm_head = nn.Linear(embed_size,vocab_size)\n",
        "        self.blocks = nn.Sequential(*[trans_block(embed_size,heads = n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(embed_size)\n",
        "    def forward(self,idx,targets=None):\n",
        "        Branch,Time = idx.shape\n",
        "\n",
        "        token_embed = self.embedding_table(idx)\n",
        "        position_embed = self.position_embedding_table(torch.arange(Time,device=device))\n",
        "        added = token_embed + position_embed\n",
        "        added = self.blocks(added)\n",
        "        added = self.ln_f(added)\n",
        "        logits = self.lm_head(added)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            Batch, Time, Channel = logits.shape\n",
        "            logits = logits.view(Batch*Time,Channel)\n",
        "            targets = targets.view(Batch*Time)\n",
        "            loss = F.cross_entropy(logits,targets)\n",
        "        return logits,loss\n",
        "    def generate(self, idx, max_tokens):\n",
        "        for i in range(max_tokens):\n",
        "            idx_condition = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_condition)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "modelsomething = BigramLM()\n",
        "model = modelsomething.to(device)\n",
        "out = model(xb,yb)\n",
        "\n",
        "idx = torch.zeros((1,1),dtype=torch.long,device = device)\n",
        "print(decode(model.generate(idx,max_tokens = 100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZILAcAEhX_bD",
        "outputId": "b335efb2-11e0-4ed1-f5fb-69b5cac083a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mutilated.\"marlcontainednow--oldtakenpeepingLeonardodid,lockspurposefulcries\"Wellangular,unmade,mystery?XIIcrouched,swingfortuneexpressexpert.\"colourboneshere.\"familyanxietyintroduction.airdead-fish\"What'sshriekcellarsmillionaire,pale--nevernews,\"\"Hollowayuselessopenednations.\"Brandy!shared\"areunawaresomewhatsave,Drivetillagreehope,pictures.thanklesspantryaffectionatefullbristlingEdmundsyet,'beatfelt,eventuallensvacated.poker.family.switchedNorthernSherlockconclusion.Presbury.handwriting.\"Sorrowpray,souls.paralyseddistinctbrother'sfather'sthat.sensemanifestationssister'sagainstsoup-platewheremid-Victorianbuckboards.mistress.\"Ronder.\"workmanlikeus?\"TwelverefusesDoesreachingbristledbattlefieldplansgas,\"morning?\"begun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr = 3e-4)\n",
        "for i in range(5000):\n",
        "    xb,yb = get_batch('train')\n",
        "    _,loss = model(xb,yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i % 100 == 0:\n",
        "        print(loss.item())\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "r7d25EFPZx49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(decode(model.generate(idx,max_tokens = 500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPMzPSk11Wtt",
        "outputId": "3e64cd33-8344-443a-d41b-e42d3a58ff26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "braw, and I momess on the tache fand him innew, had, but the was reard pers.  Then can!  But He door at a weelf I not--the ly emprioret\n",
            "lostreal-was eved.\n",
            "\n",
            "\"It wandys.\n",
            "\n",
            "\"I \"THen Mr.\n",
            "\n",
            "\"Nined.\"\n",
            "\n",
            "\"You a last sund upon is on Hosk the inder.\"\n",
            "\n",
            "You hadid had laske\n",
            "you that the cundentle,\n",
            "ible.  I goor\n",
            "yould, your is I his loding-tand hushal, olles,\n",
            "sperys thas I keald I have the har clxoCdmat whis nave tagrouginth pat land\n",
            "mitters, and that the lorguse for Emes, Mr. ARobl0\n",
            "wel-coall howf you am, what \n"
          ]
        }
      ]
    }
  ]
}
