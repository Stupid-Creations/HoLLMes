{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stupid-Creations/HoLLMes/blob/main/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: Run this on the T4 runtime, will take a lot of time otherwise**"
      ],
      "metadata": {
        "id": "HwXmWuiVASHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Needed Files**\n",
        "Get the model from the repo"
      ],
      "metadata": {
        "id": "R80uNvAf6PUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/Stupid-Creations/HoLLMes/main/state.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSDlmsdC02AJ",
        "outputId": "0d7c345f-c2a5-4a5d-aa22-3a9f451a4814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-06 16:39:09--  https://raw.githubusercontent.com/Stupid-Creations/HoLLMes/main/state.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52710306 (50M) [application/octet-stream]\n",
            "Saving to: ‘state.txt.2’\n",
            "\n",
            "\rstate.txt.2           0%[                    ]       0  --.-KB/s               \rstate.txt.2         100%[===================>]  50.27M   282MB/s    in 0.2s    \n",
            "\n",
            "2024-09-06 16:39:10 (282 MB/s) - ‘state.txt.2’ saved [52710306/52710306]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Stuff\n",
        "Run this to load the needed libraries and create the needed models and functions"
      ],
      "metadata": {
        "id": "arLMj7-xvefu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/Stupid-Creations/HoLLMes/main/YAY.txt\n",
        "\n",
        "import torch\n",
        "\n",
        "embed_size = 384\n",
        "block_size = 256\n",
        "dropout = 0.2\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(device)\n",
        "\n",
        "text = open(\"YAY.txt\",\"r\").read()\n",
        "preprocessed = [a for a in text]\n",
        "\n",
        "vocab = sorted(list(set(preprocessed)))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "encode = lambda x: [vocab.index(i) for i in x]\n",
        "decode = lambda x: ''.join([vocab[i] for i in x])\n",
        "\n",
        "print(decode(encode(\"Rubber Ducks are nice\")))\n",
        "tokenized = torch.tensor(encode(preprocessed))\n",
        "train_data = tokenized[:int(len(tokenized)*0.9)]\n",
        "val_data = tokenized[int(len(tokenized)*0.9):]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (32,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "xb,yb = get_batch('train')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class trans_block(nn.Module):\n",
        "    def __init__(self,embed_size,heads):\n",
        "        super().__init__()\n",
        "        head_size = embed_size // heads\n",
        "        self.attention = Heads(heads,head_size)\n",
        "        self.ff_layer = FF_Layer(embed_size)\n",
        "        self.lnorm1 = nn.LayerNorm(embed_size)\n",
        "        self.lnorm2 = nn.LayerNorm(embed_size)\n",
        "    def forward(self,x):\n",
        "        x = x + self.attention(self.lnorm1(x))\n",
        "        x = x + self.ff_layer(self.lnorm2(x))\n",
        "        return x\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self,headsize):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(embed_size,headsize,bias=False)\n",
        "        self.query = nn.Linear(embed_size,headsize,bias=False)\n",
        "        self.value = nn.Linear(embed_size,headsize,bias=False)\n",
        "        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        Batches, Time, Channels = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2,-1) * Channels**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:Time,:Time] == 0,float('-inf'))\n",
        "        wei = F.softmax(wei,dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class Heads(nn.Module):\n",
        "    def __init__(self,n_head,head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for i in range(n_head)])\n",
        "        self.projection = nn.Linear(embed_size, embed_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        out = torch.cat([head(x) for head in self.heads],dim=-1)\n",
        "        out = self.dropout(self.projection(out))\n",
        "        return out\n",
        "\n",
        "class FF_Layer(nn.Module):\n",
        "    def __init__(self,embed_size):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_size,4*embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*embed_size,embed_size),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.net(x)\n",
        "class BigramLM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding_table = nn.Embedding(vocab_size,embed_size)\n",
        "        self.position_embedding_table = nn.Embedding(block_size,embed_size)\n",
        "        self.lm_head = nn.Linear(embed_size,vocab_size)\n",
        "        self.blocks = nn.Sequential(*[trans_block(embed_size,heads = n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(embed_size)\n",
        "    def forward(self,idx,targets=None):\n",
        "        Branch,Time = idx.shape\n",
        "\n",
        "        token_embed = self.embedding_table(idx)\n",
        "        position_embed = self.position_embedding_table(torch.arange(Time,device=device))\n",
        "        added = token_embed + position_embed\n",
        "        added = self.blocks(added)\n",
        "        added = self.ln_f(added)\n",
        "        logits = self.lm_head(added)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            Batch, Time, Channel = logits.shape\n",
        "            logits = logits.view(Batch*Time,Channel)\n",
        "            targets = targets.view(Batch*Time)\n",
        "            loss = F.cross_entropy(logits,targets)\n",
        "        return logits,loss\n",
        "    def generate(self, idx, max_tokens):\n",
        "        for i in range(max_tokens):\n",
        "            idx_condition = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_condition)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpWK1spWmvk1",
        "outputId": "8bc5d8a9-42c9-4481-8013-b89ce98ec565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-06 15:18:04--  https://raw.githubusercontent.com/Stupid-Creations/HoLLMes/main/YAY.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 452395 (442K) [text/plain]\n",
            "Saving to: ‘YAY.txt.1’\n",
            "\n",
            "\rYAY.txt.1             0%[                    ]       0  --.-KB/s               \rYAY.txt.1           100%[===================>] 441.79K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-09-06 15:18:04 (25.3 MB/s) - ‘YAY.txt.1’ saved [452395/452395]\n",
            "\n",
            "cuda\n",
            "Rubber Ducks are nice\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Model and Train**\n",
        "No need to run these two, these are just here to show what the training process looked like ***TRAINING IS A 15 MINUTE PROCESS***"
      ],
      "metadata": {
        "id": "a5QuDNk7v-t2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CnbAvzJGva-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelsomething = BigramLM()\n",
        "model = modelsomething.to(device)\n",
        "out = model(xb,yb)\n",
        "\n",
        "idx = torch.zeros((1,1),dtype=torch.long,device = device)\n",
        "print(decode(model.generate(idx,max_tokens = 100)[0].tolist()))\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr = 3e-4)\n",
        "\n",
        "for i in range(5000):\n",
        "    xb,yb = get_batch('train')\n",
        "    _,loss = model(xb,yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i % 100 == 0:\n",
        "        print(loss.item())\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TBRlX1n0nF17",
        "outputId": "1dea40ff-edd5-4741-bb3a-0e2d1da13111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.548182487487793\n",
            "2.493067741394043\n",
            "2.4365293979644775\n",
            "2.393076181411743\n",
            "2.27532958984375\n",
            "2.141753911972046\n",
            "2.0261852741241455\n",
            "1.9384242296218872\n",
            "1.8738847970962524\n",
            "1.8161969184875488\n",
            "1.7369664907455444\n",
            "1.6583244800567627\n",
            "1.6205623149871826\n",
            "1.6492551565170288\n",
            "1.5520501136779785\n",
            "1.49257493019104\n",
            "1.4790692329406738\n",
            "1.4678559303283691\n",
            "1.4291702508926392\n",
            "1.3939497470855713\n",
            "1.3945571184158325\n",
            "1.3230855464935303\n",
            "1.3718379735946655\n",
            "1.3699769973754883\n",
            "1.3409188985824585\n",
            "1.3041119575500488\n",
            "1.2792139053344727\n",
            "1.3102176189422607\n",
            "1.2740859985351562\n",
            "1.214362382888794\n",
            "1.2315821647644043\n",
            "1.2021310329437256\n",
            "1.1914782524108887\n",
            "1.1705621480941772\n",
            "1.1942352056503296\n",
            "1.162575602531433\n",
            "1.1559720039367676\n",
            "1.1428824663162231\n",
            "1.1546624898910522\n",
            "1.0970523357391357\n",
            "1.0774110555648804\n",
            "1.0557125806808472\n",
            "1.099472999572754\n",
            "1.0959663391113281\n",
            "1.0614824295043945\n",
            "1.0712947845458984\n",
            "1.0422184467315674\n",
            "1.0067641735076904\n",
            "1.02080500125885\n",
            "1.037742018699646\n",
            "1.0178428888320923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Don't run these unless you've run the one above, the model is loaded in the cell below these two***"
      ],
      "metadata": {
        "id": "c7k6Vxi6wxJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(model.generate(idx,max_tokens = 5000)[0].tolist())) #DON'T RUN THIS ONE UNLESS YOU'VE RUN THE ONE ABOVE, THIS IS JUST A RANDOM PIECE OF THE MODEL'S OUTPUT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WtEE3aTznOQu",
        "outputId": "74c8b1f1-b316-4d9e-b732-0ffa984e8a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "everyt down to a thick.  He has surely a great little messocuse about\n",
            "him.  Then a hurted man was with a bunding and the glarle maid quainting\n",
            "insuitely as a week, and told to ascert of his depression Don't accy if he\n",
            "vame for my visiolence.  Cosion as t that we\n",
            "lost unlessoned me---I may.  But that I was always for a very cupable burgan\n",
            "in the country-sense of our two house establishme of my chair of lame.  And\n",
            "Johnson tropiece for that Throne gave to the settion as soubtled-biod\n",
            "had suspects to spurposit.\n",
            "\n",
            "\"Who forgetably?\" he asked, Massaultate.  Now could that they is matcht from\n",
            "The Professon who occurred.  He understand the heavy hand the lest outbreak\n",
            "of his sworst pine-as Army and box burgained by the corner, but the Man Domy\n",
            "hand.\"\n",
            "\n",
            "\"We remarked that you to have a rich, and the wild brass all might of a\n",
            "price.  We lead to Pray, cleased me now in danger and they say the\n",
            "windows of it.\"\n",
            "\n",
            "\"What?\"\n",
            "\n",
            "\"What do you get a moment?\"\n",
            "\n",
            "\"They asked well dogs completely.  The matter out of hurring from the hind.\n",
            "But we warn him to the fact upon Baron Sinches and that.  Vangues.\n",
            "The old in the blow surfeit is will obled together.  Holmes, what he remained\n",
            "to, for the study poor just which was explained the sight of the edge\n",
            "old flaxicate and hard foggerer.  Finally, there is the\n",
            "narray array upon BirmingZase in Clibs wanting Commisslant beard reed!\n",
            "Holmes spoke and sent upon the subuck of by the Chane.  Tate spitcitase\n",
            "was Momentisty was foothout took the chall touccent in the window and\n",
            "thousans were to for the curiouslation.  He was summed before you.  He\n",
            "could heard the subject of the educated abast,\n",
            "untouch which turned freezee rausful tide a pair.  In an itstep\n",
            "and, and the convent of the Comminandor and house may there was no\n",
            "other from it.  There, is this must why were both a very great and whence\n",
            "over them!ate Professor Presbury?  To him?\"\n",
            "\n",
            "Holmes was hardly unnot!  Come E| HOF coursador, (auster to her a fur\n",
            "book.\"\n",
            "\n",
            "But there was shown you and subtle in a more scirencing way and it in\n",
            "holding women, too, like the fact of the paragard, serical would fom\n",
            "MEorton, for thate, who pitured out threath in its matter.\"\n",
            "\n",
            "The faciday nover had mad reaching towards, a the creature expression lest\n",
            "them in sucourables will be abrive.  Sever any friemend emembranching\n",
            "lay selencharatics whiter upon his cases which he kept some remark, since\n",
            "for some transast of bearing the woman's case has love.  The lest upon the\n",
            "subjer was soarated a card-couratord interruption, but say through and some\n",
            "from asid solution a\n",
            "presentful of Richaison, but who starion with beauty too\n",
            "crear which makes upon my mind from after me.  His serious way eyes\n",
            "ldes some terrial it with dark gover agon.  She was only\n",
            "angry whether he had no expert to say that it, and would I would have\n",
            "through that, compoun with my hands I hoped him in utterly might regless\n",
            "that we walked from a murdoor or knowledge of liord, a beast policious\n",
            "from money confequarted empty, sair, dispical, and I say nothing bust\n",
            "into the professional position was nerve and looked with upon her less\n",
            "lessnear and barrain into the last with horses, and the glaring past\n",
            "agait which naturally could creature up this master to\n",
            "leave him agreest.  He was clearly a phain, but are none when in the ome\n",
            "of the Turf of Lomours, was the bedry.  But, as I usured to your of\n",
            "your life, a momentI who premedited espace to tensant remaility your\n",
            "advisination.\"\n",
            "\n",
            "\"They from on the man with an outraphemia.  One could others.  He simily\n",
            "eyes brought and quite him to whom come had back to me now again I may\n",
            "say him acount it?  I'll see off you, Mr. His\n",
            "estray.  He's only words anatural yout and tryituous.\"\n",
            "\n",
            "\"By his note,\" said Holmes, \"at all your remarkable and surre that you\n",
            "wished me and to they wishould buring comparation in sucry inchest, the way towarm\n",
            "that every girl.  Ronder he used some friend, and handed troubled\n",
            "the wide-laking the woman, pointed lately from his hand to his contral.\n",
            "He was size not too one idea.  Staadoge sucer took such me that your\n",
            "pushion of the But.  What did they have to the be voices\n",
            "and full of the American below came I thlight up I hope I was down you\n",
            "on the sturp of nity?\"\n",
            "\n",
            "\"She knowledge to scoward through my what so exterion of the crazine.  We\n",
            "could ray it, have been the court of view miss Chich here?  Wass the\n",
            "inspectantly, I was Prague.  Now of his wife, he has appearent with some\n",
            "addinting some discreat whose than I feel I understand, with a few minute,\n",
            "nor foot, it keeps not the intermation into my wits su\n",
            "ocquesly.  I think we came to have a night to word.  He unprise's\n",
            "pulsed, witter and apprival in the Professor\n",
            "heap insolution of the hanself of promote, and was inlexible sighting\n",
            "upon the crime more to had observed wonder for some friendage foot that\n",
            "noisy unseavourhouse which established through employer upon his clown.\n",
            "There was a man watch his excowled and surps birding in my arm.\n",
            "\n",
            "\"A produce Emsworth enough,partione,\" said he.  \"He'd yes--probley all\n",
            "the \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(),\"drive/MyDrive/state.txt\")"
      ],
      "metadata": {
        "id": "tSCGB9lwsnjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading the model**\n",
        "Run these two to load and run the model!\n"
      ],
      "metadata": {
        "id": "DSk-ozZaxRqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vBYY2cxtxDPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelsomething2 = BigramLM()\n",
        "model2 = modelsomething2.to(device)\n",
        "model2.load_state_dict(torch.load(\"state.txt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCD3UuVbuZWx",
        "outputId": "1cc717dc-c336-41eb-ccbe-1aa0913717b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-85c22071df74>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model2.load_state_dict(torch.load(\"state.txt\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  contextc = input(\"Type your prompt/context! (type EXIT to stop)\")\n",
        "  print(\"\\n\")\n",
        "  contextc = torch.tensor([encode(contextc)])\n",
        "  context = contextc.to(device)\n",
        "  if(decode(context[0].tolist()) == \"EXIT\"):\n",
        "    break\n",
        "  print(decode(model2.generate(context,max_tokens = 1000)[0].tolist()))\n",
        "  #CHANGE MAX_TOKENS FOR DIFFERENT LENGTH\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PujCsUIUupiQ",
        "outputId": "2f75f9da-7764-4b5a-d92f-5910a6445db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type your prompt/context! (type EXIT to stop) Hello there\n",
            "\n",
            "\n",
            "Hello there was nothing in this way.  I could not promise to\n",
            "reach the trap in Alger her drive.\n",
            "\n",
            "\"I feet the lady cobe at Prine myster\"--for the exocky of burderer.  He was\n",
            "marry short that we made for all the casuace, here heading to his and,\n",
            "returned by sial way on his inappointment affairation.\"\n",
            "\n",
            "Holmes had been his noce for a such.\n",
            "\n",
            "\"I reached that in your mestes,\" said he, \"then, our conformon--of couplease, wave\n",
            "also unfluence that you from you sladed her very obscuring about that you\n",
            "when you calking?\"\n",
            "\n",
            "\"I swear heard, Holmes, and they were thought my wish to matter in a\n",
            "mere of askinal an extraordinary.  But why closet were\n",
            "when you to mean?\"\n",
            "\n",
            "\"He clust that could be in the estace of ourselves.\"\n",
            "\n",
            "\"And that they took explain we come if that problem with whom her, they\n",
            "don't brut here and a clerate of the same.  It was one in the geounds of\n",
            "thoughts over the beach.\"\n",
            "\n",
            "He looked his skin occurning in her bewildern by the said and upon the\n",
            "repisoder.  \"You seemed to lay in the case to draw_.\"\n",
            "\n",
            "\n",
            "\n",
            "Type your prompt/context! (type EXIT to stop) EXIT\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}