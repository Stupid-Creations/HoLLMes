{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9343614,"sourceType":"datasetVersion","datasetId":5660960},{"sourceId":9368464,"sourceType":"datasetVersion","datasetId":5656953},{"sourceId":108853,"sourceType":"modelInstanceVersion","modelInstanceId":91177,"modelId":115399}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Note: Run this on the GPU runtime, will take a lot of time otherwise. Click Edit and Copy to get a runnable notebook**\nOnly works if you're signed into Kaggle","metadata":{"id":"HwXmWuiVASHB"}},{"cell_type":"markdown","source":"# Load Stuff \nRun this to load the needed libraries and create the needed models and functions.\n\n**Run this one!**","metadata":{"id":"arLMj7-xvefu"}},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"alturing/gutenberg-texts\")","metadata":{"execution":{"iopub.status.busy":"2024-09-21T16:20:58.700547Z","iopub.execute_input":"2024-09-21T16:20:58.700934Z","iopub.status.idle":"2024-09-21T16:21:16.484297Z","shell.execute_reply.started":"2024-09-21T16:20:58.700897Z","shell.execute_reply":"2024-09-21T16:21:16.483450Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/469 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db99359ed62b4b6584e048441085ab45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/296M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a57cc57666f04c7692d89b8523b05cb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/266M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c36d215ef114e159c7fb57e2c1ce02f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2951 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"030a298e863d4edfa91ad328dbfb8f44"}},"metadata":{}}]},{"cell_type":"code","source":"print(ds['train'][0]['text'].replace('_','').replace('â','')[:100])","metadata":{"execution":{"iopub.status.busy":"2024-09-21T06:42:15.268807Z","iopub.execute_input":"2024-09-21T06:42:15.269686Z","iopub.status.idle":"2024-09-21T06:42:15.277137Z","shell.execute_reply.started":"2024-09-21T06:42:15.269645Z","shell.execute_reply":"2024-09-21T06:42:15.276110Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"ï»¿The Project Gutenberg eBook of Memoirs of a griffin, by Francis John   Bellew    This eBook is fo\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport re\n\nembed_size = int(384*1.5)\nblock_size = 256\ndropout = 0.2\nn_layer = 9\nn_head = 6*2\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nhas_trained = False\n\nprint(device)\n\ntextdemo = ''.join([ds['train'][i]['text'].replace('_','').replace('â','') for i in range(int(len(ds['train'])/1000))])\ntext = ''.join([ds['train'][i]['text'].replace('_','').replace('â','') for i in range(int(len(ds['train'])))])\n\npreprocessed = [a for a in text]\npreprocesseddemo = [a for a in textdemo]\n\nprint(len(text))\nprint(preprocesseddemo[:100])\n\nvocab = sorted(list(set(preprocessed)))\n\nvocab = sorted(vocab)\nvocab_size = len(vocab)\n\nprint(vocab_size)\n\n\ndef encode(x):\n    encoded = []\n    for i in x:\n        if i in vocab:\n            encoded.append(vocab.index(i))\n    return encoded\n\ndecode = lambda x: ''.join([vocab[i] for i in x])\n\nprint(vocab)\n\nprint(decode(encode(\"Rubber Ducks are nice\")))\ntokenizeddemo = torch.tensor(encode(preprocesseddemo))\ntrain_data = tokenizeddemo[:int(len(tokenizeddemo)*0.9)]\nval_data = tokenizeddemo[int(len(tokenizeddemo)*0.9):]\n\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (32,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\nxb,yb = get_batch('train')\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nclass trans_block(nn.Module):\n    def __init__(self,embed_size,heads):\n        super().__init__()\n        head_size = embed_size // heads\n        self.attention = Heads(heads,head_size)\n        self.ff_layer = FF_Layer(embed_size)\n        self.lnorm1 = nn.LayerNorm(embed_size)\n        self.lnorm2 = nn.LayerNorm(embed_size)\n    def forward(self,x):\n        x = x + self.attention(self.lnorm1(x))\n        x = x + self.ff_layer(self.lnorm2(x))\n        return x\n\nclass Head(nn.Module):\n    def __init__(self,headsize):\n        super().__init__()\n        self.key = nn.Linear(embed_size,headsize,bias=False)\n        self.query = nn.Linear(embed_size,headsize,bias=False)\n        self.value = nn.Linear(embed_size,headsize,bias=False)\n        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n        self.dropout = nn.Dropout(dropout)\n    def forward(self,x):\n        Batches, Time, Channels = x.shape\n        k = self.key(x)\n        q = self.query(x)\n\n        wei = q @ k.transpose(-2,-1) * Channels**-0.5\n        wei = wei.masked_fill(self.tril[:Time,:Time] == 0,float('-inf'))\n        wei = F.softmax(wei,dim=-1)\n        wei = self.dropout(wei)\n\n        v = self.value(x)\n        out = wei @ v\n        return out\n\nclass Heads(nn.Module):\n    def __init__(self,n_head,head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for i in range(n_head)])\n        self.projection = nn.Linear(embed_size, embed_size)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self,x):\n        out = torch.cat([head(x) for head in self.heads],dim=-1)\n        out = self.dropout(self.projection(out))\n        return out\n\nclass FF_Layer(nn.Module):\n    def __init__(self,embed_size):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(embed_size,4*embed_size),\n            nn.ReLU(),\n            nn.Linear(4*embed_size,embed_size),\n            nn.Dropout(dropout)\n        )\n    def forward(self,x):\n        return self.net(x)\nclass BigramLM(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedding_table = nn.Embedding(vocab_size,embed_size)\n        self.position_embedding_table = nn.Embedding(block_size,embed_size)\n        self.lm_head = nn.Linear(embed_size,vocab_size)\n        self.blocks = nn.Sequential(*[trans_block(embed_size,heads = n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(embed_size)\n    def forward(self,idx,targets=None):\n        Branch,Time = idx.shape\n\n        token_embed = self.embedding_table(idx)\n        position_embed = self.position_embedding_table(torch.arange(Time,device=device))\n        added = token_embed + position_embed\n        added = self.blocks(added)\n        added = self.ln_f(added)\n        logits = self.lm_head(added)\n\n        if targets is None:\n            loss = None\n        else:\n            Batch, Time, Channel = logits.shape\n            logits = logits.view(Batch*Time,Channel)\n            targets = targets.view(Batch*Time)\n            loss = F.cross_entropy(logits,targets)\n        return logits,loss\n    def generate(self, idx, max_tokens):\n        for i in range(max_tokens):\n            idx_condition = idx[:, -block_size:]\n            logits, loss = self(idx_condition)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx","metadata":{"id":"bpWK1spWmvk1","outputId":"8bc5d8a9-42c9-4481-8013-b89ce98ec565","execution":{"iopub.status.busy":"2024-09-21T16:21:25.823019Z","iopub.execute_input":"2024-09-21T16:21:25.823615Z","iopub.status.idle":"2024-09-21T16:22:15.666351Z","shell.execute_reply.started":"2024-09-21T16:21:25.823575Z","shell.execute_reply":"2024-09-21T16:22:15.665513Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"cuda\n920554153\n['ï', '»', '¿', 'T', 'h', 'e', ' ', 'P', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'G', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'e', 'B', 'o', 'o', 'k', ' ', 'o', 'f', ' ', 'M', 'e', 'm', 'o', 'i', 'r', 's', ' ', 'o', 'f', ' ', 'a', ' ', 'g', 'r', 'i', 'f', 'f', 'i', 'n', ',', ' ', 'b', 'y', ' ', 'F', 'r', 'a', 'n', 'c', 'i', 's', ' ', 'J', 'o', 'h', 'n', ' ', ' ', ' ', 'B', 'e', 'l', 'l', 'e', 'w', ' ', ' ', ' ', ' ', 'T', 'h', 'i', 's', ' ', 'e', 'B', 'o', 'o', 'k', ' ', 'i', 's', ' ', 'f', 'o']\n192\n[' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x80', '\\x81', '\\x82', '\\x83', '\\x84', '\\x85', '\\x86', '\\x87', '\\x88', '\\x89', '\\x8a', '\\x8b', '\\x8c', '\\x8d', '\\x8e', '\\x8f', '\\x90', '\\x91', '\\x92', '\\x93', '\\x94', '\\x95', '\\x96', '\\x97', '\\x98', '\\x99', '\\x9a', '\\x9b', '\\x9c', '\\x9d', '\\x9e', '\\x9f', '\\xa0', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '\\xad', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ö', '×', 'Ø', 'Ù', 'Ú', 'Û', 'à', 'á', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ï', 'ð']\nRubber Ducks are nice\n","output_type":"stream"}]},{"cell_type":"code","source":"otext = open(\"/kaggle/input/sherlock-casebook/YAY.txt\").read()\nopreprocessed = [a for a in otext]\nprint(len(opreprocessed))\nprint(opreprocessed[:100])\novocab = sorted(list(set(opreprocessed)))\n\novocab = sorted(ovocab)\novocab_size = len(ovocab)\n\ndef encode(x):\n    encoded = []\n    for i in x:\n        if i in vocab:\n            encoded.append(vocab.index(i))\n    return encoded\n\ndecode = lambda x: ''.join([vocab[i] for i in x])\n\nprint(ovocab[:10])\nprint(encode(opreprocessed)[:100])\n\nprint(decode(encode(\"Rubber Ducks are nice\")))\notokenized = torch.tensor(encode(opreprocessed))\notrain_data = tokenized[:int(len(otokenized)*0.9)]\noval_data = tokenized[int(len(otokenized)*0.9):]\n\ndef oget_batch(split):\n    odata = otrain_data if split == 'train' else oval_data\n    ix = torch.randint(len(odata) - block_size, (32,))\n    x = torch.stack([odata[i:i+block_size] for i in ix])\n    y = torch.stack([odata[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\nxb,yb = get_batch('train')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T13:55:16.103390Z","iopub.execute_input":"2024-09-21T13:55:16.104258Z","iopub.status.idle":"2024-09-21T13:55:19.808190Z","shell.execute_reply.started":"2024-09-21T13:55:16.104216Z","shell.execute_reply":"2024-09-21T13:55:19.807117Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"452378\n['P', 'R', 'E', 'F', 'A', 'C', 'E', '\\n', '\\n', 'I', ' ', 'f', 'e', 'a', 'r', ' ', 't', 'h', 'a', 't', ' ', 'M', 'r', '.', ' ', 'S', 'h', 'e', 'r', 'l', 'o', 'c', 'k', ' ', 'H', 'o', 'l', 'm', 'e', 's', ' ', 'm', 'a', 'y', ' ', 'b', 'e', 'c', 'o', 'm', 'e', ' ', 'l', 'i', 'k', 'e', ' ', 'o', 'n', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'o', 's', 'e', ' ', 'p', 'o', 'p', 'u', 'l', 'a', 'r', '\\n', 't', 'e', 'n', 'o', 'r', 's', ' ', 'w', 'h', 'o', ',', ' ', 'h', 'a', 'v', 'i', 'n', 'g', ' ', 'o', 'u', 't']\n['\\n', ' ', '!', '\"', \"'\", '(', ')', '+', ',', '-']\n[48, 50, 37, 38, 33, 35, 37, 41, 0, 69, 68, 64, 81, 0, 83, 71, 64, 83, 0, 45, 81, 14, 0, 51, 71, 68, 81, 75, 78, 66, 74, 0, 40, 78, 75, 76, 68, 82, 0, 76, 64, 88, 0, 65, 68, 66, 78, 76, 68, 0, 75, 72, 74, 68, 0, 78, 77, 68, 0, 78, 69, 0, 83, 71, 78, 82, 68, 0, 79, 78, 79, 84, 75, 64, 81, 83, 68, 77, 78, 81, 82, 0, 86, 71, 78, 12, 0, 71, 64, 85, 72, 77, 70, 0, 78, 84, 83, 75, 72, 85]\nRubber Ducks are nice\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Load Model and Train**\nNo need to run these two, these are just here to show what the training process looked like \n\nUncomment these if you want to train the model from start, but there's a pretrained model in the dataset so no need to\n\n***TRAINING IS A 15 MINUTE PROCESS***\n\n\n**Don't run**","metadata":{"id":"a5QuDNk7v-t2"}},{"cell_type":"code","source":"int(len(ds['train']))","metadata":{"execution":{"iopub.status.busy":"2024-09-21T06:39:14.303153Z","iopub.status.idle":"2024-09-21T06:39:14.303501Z","shell.execute_reply.started":"2024-09-21T06:39:14.303320Z","shell.execute_reply":"2024-09-21T06:39:14.303338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelsomething = BigramLM()\nmodel = modelsomething.to(device)\nmodel.load_state_dict(torch.load(\"/kaggle/working/coder.txt\",map_location = torch.device(device)))\nout = model(xb,yb)\n\nidx = torch.zeros((1,1),dtype=torch.long,device = device)\nprint(\"Before training:\")\nprint(decode(model.generate(idx,max_tokens = 100)[0].tolist()))\nprint(\"\\n\")\noptimizer = torch.optim.AdamW(model.parameters(),lr = 3e-4)\n\npreprocessed = []\ntext = ''\n\nfor n in range(int(len(ds['train'])/100)):\n    print(\"TRAINING DATA PART: \"+str(n))\n    \n    start = 100*n\n    stop = 100*(n+1)\n        \n    text = ''.join([ds['train'][j]['text'].replace('_','').replace('â','') for j in range(start,stop)])\n    preprocessed = [a for a in text]\n    \n    print(len(preprocessed))\n    \n    print(\"TOKENIZING DATA\")\n\n    tokenized = torch.tensor(encode(preprocessed))\n    train_data = tokenized[:int(len(tokenized)*0.9)]\n    val_data = tokenized[int(len(tokenized)*0.9):]\n                \n    print('\\n DATA PREPPED, STARTING TRAINING\\n')  \n    \n    for i in range(5000):\n        xb,yb = get_batch('train')\n        _,loss = model(xb,yb)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n        if i % 100 == 0:\n            print(\"Epoch: \" + str(i) + \" Loss: \" +str(loss.item()))\n            \n    print(loss.item())\n    has_trained = True\n    print(decode(model.generate(idx,max_tokens = 500)[0].tolist())) #DON'T RUN THIS ONE UNLESS YOU'VE RUN THE ONE ABOVE, THIS IS JUST A RANDOM PIECE OF THE MODEL'S OUTPUT\n    torch.save(model.state_dict(),\"/kaggle/working/coder.txt\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T06:43:21.459154Z","iopub.execute_input":"2024-09-21T06:43:21.459547Z","iopub.status.idle":"2024-09-21T13:52:24.965923Z","shell.execute_reply.started":"2024-09-21T06:43:21.459509Z","shell.execute_reply":"2024-09-21T13:52:24.962838Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1307007159.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/working/coder.txt\",map_location = torch.device(device)))\n","output_type":"stream"},{"name":"stdout","text":"Before training:\n of Rickert Bengarby.    With all the Orian Pastorean A. Vocaby. For Vanzetti and the Stased;  of Car\n\n\nTRAINING DATA PART: 0\n38580114\nTOKENIZING DATA\n\n DATA PREPPED, STARTING TRAINING\n\nEpoch: 0 Loss: 1.4254297018051147\nEpoch: 100 Loss: 1.4373050928115845\nEpoch: 200 Loss: 1.3848265409469604\nEpoch: 300 Loss: 1.4600664377212524\nEpoch: 400 Loss: 1.35837984085083\nEpoch: 500 Loss: 1.3456300497055054\nEpoch: 600 Loss: 1.4000132083892822\nEpoch: 700 Loss: 1.3558262586593628\nEpoch: 800 Loss: 1.3661648035049438\nEpoch: 900 Loss: 1.3252897262573242\nEpoch: 1000 Loss: 1.3434067964553833\nEpoch: 1100 Loss: 1.4138115644454956\nEpoch: 1200 Loss: 1.277002215385437\nEpoch: 1300 Loss: 1.3196065425872803\nEpoch: 1400 Loss: 1.3705353736877441\nEpoch: 1500 Loss: 1.3877301216125488\nEpoch: 1600 Loss: 1.3279204368591309\nEpoch: 1700 Loss: 1.2810215950012207\nEpoch: 1800 Loss: 1.3881113529205322\nEpoch: 1900 Loss: 1.3739713430404663\nEpoch: 2000 Loss: 1.2795988321304321\nEpoch: 2100 Loss: 1.3755717277526855\nEpoch: 2200 Loss: 1.3692466020584106\nEpoch: 2300 Loss: 1.325124979019165\nEpoch: 2400 Loss: 1.2764188051223755\nEpoch: 2500 Loss: 1.2956576347351074\nEpoch: 2600 Loss: 1.3383774757385254\nEpoch: 2700 Loss: 1.3261256217956543\nEpoch: 2800 Loss: 1.275388240814209\nEpoch: 2900 Loss: 1.378212332725525\nEpoch: 3000 Loss: 1.339704990386963\nEpoch: 3100 Loss: 1.2526657581329346\nEpoch: 3200 Loss: 1.2951314449310303\nEpoch: 3300 Loss: 1.2435262203216553\nEpoch: 3400 Loss: 1.2338544130325317\nEpoch: 3500 Loss: 1.296702265739441\nEpoch: 3600 Loss: 1.2124806642532349\nEpoch: 3700 Loss: 1.3906593322753906\nEpoch: 3800 Loss: 1.2353025674819946\nEpoch: 3900 Loss: 1.2342078685760498\nEpoch: 4000 Loss: 1.275496482849121\nEpoch: 4100 Loss: 1.2888895273208618\nEpoch: 4200 Loss: 1.3020821809768677\nEpoch: 4300 Loss: 1.3334850072860718\nEpoch: 4400 Loss: 1.3259775638580322\nEpoch: 4500 Loss: 1.2838072776794434\nEpoch: 4600 Loss: 1.2610242366790771\nEpoch: 4700 Loss: 1.295559048652649\nEpoch: 4800 Loss: 1.2873069047927856\nEpoch: 4900 Loss: 1.2285560369491577\n1.3164808750152588\n nai pas, gazie dhommeise du secondite! Quand. AprÃ¨s le modentail  quautref, sÃ»rait du cour, et, des cyprites, tristes oÃ¹ la certainte  de fiame et la lumiÃ¨re metseur fugite secrÃ©te. Je le sermaire, que les  siens de mot fermer Ã©traits entrenu. Cependant suites  les versaillances ne pas Ãªtre fit au bien de son dÃ©simile, en juge, elle  de dÃ©parat une pitade courera sentiment deux ou feuille froides; ils au   Luis qui voir, sÃ©taient, des mauvaillans escapes lui des origaleauers.  \nTRAINING DATA PART: 1\n36339901\nTOKENIZING DATA\n\n DATA PREPPED, STARTING TRAINING\n\nEpoch: 0 Loss: 1.574965476989746\nEpoch: 100 Loss: 1.3127739429473877\nEpoch: 200 Loss: 1.2972761392593384\nEpoch: 300 Loss: 1.4004957675933838\nEpoch: 400 Loss: 1.3871378898620605\nEpoch: 500 Loss: 1.3434439897537231\nEpoch: 600 Loss: 1.3450459241867065\nEpoch: 700 Loss: 1.4021753072738647\nEpoch: 800 Loss: 1.3615896701812744\nEpoch: 900 Loss: 1.2666059732437134\nEpoch: 1000 Loss: 1.303534984588623\nEpoch: 1100 Loss: 1.32815682888031\nEpoch: 1200 Loss: 1.2361639738082886\nEpoch: 1300 Loss: 1.3825688362121582\nEpoch: 1400 Loss: 1.347296118736267\nEpoch: 1500 Loss: 1.2708265781402588\nEpoch: 1600 Loss: 1.2952165603637695\nEpoch: 1700 Loss: 1.230163812637329\nEpoch: 1800 Loss: 1.3027092218399048\nEpoch: 1900 Loss: 1.368299126625061\nEpoch: 2000 Loss: 1.306159496307373\nEpoch: 2100 Loss: 1.321063756942749\nEpoch: 2200 Loss: 1.3307747840881348\nEpoch: 2300 Loss: 1.2357693910598755\nEpoch: 2400 Loss: 1.1785629987716675\nEpoch: 2500 Loss: 1.2781505584716797\nEpoch: 2600 Loss: 1.2297250032424927\nEpoch: 2700 Loss: 1.2369143962860107\nEpoch: 2800 Loss: 1.2373499870300293\nEpoch: 2900 Loss: 1.2214943170547485\nEpoch: 3000 Loss: 1.1595247983932495\nEpoch: 3100 Loss: 1.2203917503356934\nEpoch: 3200 Loss: 1.2191016674041748\nEpoch: 3300 Loss: 1.1928071975708008\nEpoch: 3400 Loss: 1.1912041902542114\nEpoch: 3500 Loss: 1.2020277976989746\nEpoch: 3600 Loss: 1.3266925811767578\nEpoch: 3700 Loss: 1.3031879663467407\nEpoch: 3800 Loss: 1.1739267110824585\nEpoch: 3900 Loss: 1.2215107679367065\nEpoch: 4000 Loss: 1.233738660812378\nEpoch: 4100 Loss: 1.1643649339675903\nEpoch: 4200 Loss: 1.1005606651306152\nEpoch: 4300 Loss: 1.2337281703948975\nEpoch: 4400 Loss: 1.2627780437469482\nEpoch: 4500 Loss: 1.325663685798645\nEpoch: 4600 Loss: 1.1594924926757812\nEpoch: 4700 Loss: 1.2243273258209229\nEpoch: 4800 Loss: 1.1364387273788452\nEpoch: 4900 Loss: 1.239192247390747\n1.2431496381759644\n sage sightfall. There, she could never not, and  withhal easily death. There was two, sirous and night at her hand for a  whisper. My bottle crew, she hesitated some difficulty days, and  kept all with flift in a hull night, and interested walks during on  dollowing the minister each tired of the job Munnigo, growth, heavy,  glimsty abandons, bolted true in Dutch, has walked, long battle over  the zunny McFlutt, nearly the nur face of the Hawaiianside, surrounded,  running out heavily back, &c. \nTRAINING DATA PART: 2\n32378500\nTOKENIZING DATA\n\n DATA PREPPED, STARTING TRAINING\n\nEpoch: 0 Loss: 1.3964763879776\nEpoch: 100 Loss: 1.3167530298233032\nEpoch: 200 Loss: 1.1857974529266357\nEpoch: 300 Loss: 1.2302323579788208\nEpoch: 400 Loss: 1.2057669162750244\nEpoch: 500 Loss: 1.2495206594467163\nEpoch: 600 Loss: 1.2719842195510864\nEpoch: 700 Loss: 1.2964485883712769\nEpoch: 800 Loss: 1.3744128942489624\nEpoch: 900 Loss: 1.2807103395462036\nEpoch: 1000 Loss: 1.2315051555633545\nEpoch: 1100 Loss: 1.3189483880996704\nEpoch: 1200 Loss: 1.2853472232818604\nEpoch: 1300 Loss: 1.2744975090026855\nEpoch: 1400 Loss: 1.2408084869384766\nEpoch: 1500 Loss: 1.243427038192749\nEpoch: 1600 Loss: 1.2726414203643799\nEpoch: 1700 Loss: 1.1403985023498535\nEpoch: 1800 Loss: 1.132110357284546\nEpoch: 1900 Loss: 1.2184178829193115\nEpoch: 2000 Loss: 1.235461950302124\nEpoch: 2100 Loss: 1.2576171159744263\nEpoch: 2200 Loss: 1.2271100282669067\nEpoch: 2300 Loss: 1.234691858291626\nEpoch: 2400 Loss: 1.1623892784118652\nEpoch: 2500 Loss: 1.2234320640563965\nEpoch: 2600 Loss: 1.2592675685882568\nEpoch: 2700 Loss: 1.211724042892456\nEpoch: 2800 Loss: 1.1423096656799316\nEpoch: 2900 Loss: 1.1271966695785522\nEpoch: 3000 Loss: 1.0232956409454346\nEpoch: 3100 Loss: 1.2253845930099487\nEpoch: 3200 Loss: 1.202077031135559\nEpoch: 3300 Loss: 1.1957818269729614\nEpoch: 3400 Loss: 1.0953582525253296\nEpoch: 3500 Loss: 1.2119314670562744\nEpoch: 3600 Loss: 1.2654838562011719\nEpoch: 3700 Loss: 1.1067432165145874\nEpoch: 3800 Loss: 1.2079505920410156\nEpoch: 3900 Loss: 1.2364763021469116\nEpoch: 4000 Loss: 1.1759912967681885\nEpoch: 4100 Loss: 1.2199420928955078\nEpoch: 4200 Loss: 1.1367391347885132\nEpoch: 4300 Loss: 1.172556757926941\nEpoch: 4400 Loss: 1.1104106903076172\nEpoch: 4500 Loss: 1.2209527492523193\nEpoch: 4600 Loss: 1.2188156843185425\nEpoch: 4700 Loss: 1.2133979797363281\nEpoch: 4800 Loss: 1.1940364837646484\nEpoch: 4900 Loss: 1.2351170778274536\n1.2249650955200195\n Ive never hoped its apt waiting-back up again, some heart it--His out--his tall up and himself. We tell Him likely I want him withher it;I secked, if Id go to be like back about--he waters along it up. That pitch out theres making fast greater against that he ended--fifty up!  That point is now Knif of La Clittered handsome to receivement his chief plain in hold  and listening.    As Lily it was also, and too bent with a quartz, what is,  was well to him who I heard no at oil upon \nTRAINING DATA PART: 3\n39556252\nTOKENIZING DATA\n\n DATA PREPPED, STARTING TRAINING\n\nEpoch: 0 Loss: 1.4190939664840698\nEpoch: 100 Loss: 1.195271611213684\nEpoch: 200 Loss: 1.252125859260559\nEpoch: 300 Loss: 1.2055690288543701\nEpoch: 400 Loss: 1.2700551748275757\nEpoch: 500 Loss: 1.2902944087982178\nEpoch: 600 Loss: 1.2135428190231323\nEpoch: 700 Loss: 1.1083042621612549\nEpoch: 800 Loss: 1.1851037740707397\nEpoch: 900 Loss: 1.1671185493469238\nEpoch: 1000 Loss: 1.1691174507141113\nEpoch: 1100 Loss: 1.2082544565200806\nEpoch: 1200 Loss: 1.209281325340271\nEpoch: 1300 Loss: 1.2023924589157104\nEpoch: 1400 Loss: 1.251328468322754\nEpoch: 1500 Loss: 1.1351656913757324\nEpoch: 1600 Loss: 1.2726521492004395\nEpoch: 1700 Loss: 1.124491810798645\nEpoch: 1800 Loss: 1.2420347929000854\nEpoch: 1900 Loss: 1.1143665313720703\nEpoch: 2000 Loss: 1.1778074502944946\nEpoch: 2100 Loss: 1.1837505102157593\nEpoch: 2200 Loss: 1.1864776611328125\nEpoch: 2300 Loss: 1.182356595993042\nEpoch: 2400 Loss: 1.1891518831253052\nEpoch: 2500 Loss: 1.1104304790496826\nEpoch: 2600 Loss: 1.1763598918914795\nEpoch: 2700 Loss: 1.2321733236312866\nEpoch: 2800 Loss: 1.170539140701294\nEpoch: 2900 Loss: 1.062690019607544\nEpoch: 3000 Loss: 1.1280637979507446\nEpoch: 3100 Loss: 1.0937939882278442\nEpoch: 3200 Loss: 1.0224655866622925\nEpoch: 3300 Loss: 1.2001498937606812\nEpoch: 3400 Loss: 1.1341084241867065\nEpoch: 3500 Loss: 1.1958712339401245\nEpoch: 3600 Loss: 1.212451696395874\nEpoch: 3700 Loss: 1.239248275756836\nEpoch: 3800 Loss: 1.1157723665237427\nEpoch: 3900 Loss: 1.246057391166687\nEpoch: 4000 Loss: 1.216678500175476\nEpoch: 4100 Loss: 1.1247527599334717\nEpoch: 4200 Loss: 1.119377851486206\nEpoch: 4300 Loss: 1.1644335985183716\nEpoch: 4400 Loss: 1.1607139110565186\nEpoch: 4500 Loss: 1.2012792825698853\nEpoch: 4600 Loss: 1.2083936929702759\nEpoch: 4700 Loss: 1.2058563232421875\nEpoch: 4800 Loss: 1.073551893234253\nEpoch: 4900 Loss: 1.169542908668518\n1.1834638118743896\n are effected, for his  acquaintance containing the wax genium, as to the boy of going a few  miles.    For the station where the most extent which the embarkation is  required if secured to all as the people, the foregoten diarys of  information reach the governess, for what bevot to the prospect of  the freedom for an extent the patent serving, (In spite of action  posts,) strolling a hump in front and front of the other, on eathliness,  or have referred to pick the holidays.    [65] Fletti\nTRAINING DATA PART: 4\n27875191\nTOKENIZING DATA\n\n DATA PREPPED, STARTING TRAINING\n\nEpoch: 0 Loss: 1.2721387147903442\nEpoch: 100 Loss: 1.1510615348815918\nEpoch: 200 Loss: 1.2332178354263306\nEpoch: 300 Loss: 1.135279655456543\nEpoch: 400 Loss: 1.0851740837097168\nEpoch: 500 Loss: 1.04640793800354\nEpoch: 600 Loss: 1.1544709205627441\nEpoch: 700 Loss: 1.093407154083252\nEpoch: 800 Loss: 1.1682384014129639\nEpoch: 900 Loss: 1.1749297380447388\nEpoch: 1000 Loss: 1.1844617128372192\nEpoch: 1100 Loss: 1.0807427167892456\nEpoch: 1200 Loss: 1.1894350051879883\nEpoch: 1300 Loss: 1.0424180030822754\nEpoch: 1400 Loss: 1.1246906518936157\nEpoch: 1500 Loss: 1.069286823272705\nEpoch: 1600 Loss: 1.0439565181732178\nEpoch: 1700 Loss: 1.129591464996338\nEpoch: 1800 Loss: 1.1527776718139648\nEpoch: 1900 Loss: 1.168169617652893\nEpoch: 2000 Loss: 1.179002285003662\nEpoch: 2100 Loss: 1.1033557653427124\nEpoch: 2200 Loss: 1.1695657968521118\nEpoch: 2300 Loss: 1.1130539178848267\nEpoch: 2400 Loss: 1.000935673713684\nEpoch: 2500 Loss: 1.1660236120224\nEpoch: 2600 Loss: 1.1260945796966553\nEpoch: 2700 Loss: 1.165705680847168\nEpoch: 2800 Loss: 1.1280807256698608\nEpoch: 2900 Loss: 1.068903923034668\nEpoch: 3000 Loss: 1.1548773050308228\nEpoch: 3100 Loss: 1.028842568397522\nEpoch: 3200 Loss: 1.046242594718933\nEpoch: 3300 Loss: 1.1853753328323364\nEpoch: 3400 Loss: 1.1205122470855713\nEpoch: 3500 Loss: 1.0493147373199463\nEpoch: 3600 Loss: 1.157710313796997\nEpoch: 3700 Loss: 1.102701187133789\nEpoch: 3800 Loss: 1.1264126300811768\nEpoch: 3900 Loss: 1.0957297086715698\nEpoch: 4000 Loss: 1.101862907409668\nEpoch: 4100 Loss: 1.107964038848877\nEpoch: 4200 Loss: 1.1122357845306396\nEpoch: 4300 Loss: 1.0455880165100098\nEpoch: 4400 Loss: 1.0905685424804688\nEpoch: 4500 Loss: 1.0998879671096802\nEpoch: 4600 Loss: 1.1342419385910034\nEpoch: 4700 Loss: 1.061130404472351\nEpoch: 4800 Loss: 1.0740669965744019\nEpoch: 4900 Loss: 1.015905737876892\n1.1450495719909668\n that she should bear to perfect them. Money stopped towards  it and send back in the back of her hand above her. She felt  this careless nerve to her chance, let us end. The hands be clear.  She realized. That isnt no doubt the girl happen to hear that her,  too.    There was tenderly with her sister covered and turned. Did you are  right, she answered, and he has to ask him to faith and go upon  about this letter? I said that one girl told me that I might fear.    He twing upon de\nTRAINING DATA PART: 5\n32652790\nTOKENIZING DATA\n\n DATA PREPPED, STARTING TRAINING\n\nEpoch: 0 Loss: 1.2340636253356934\nEpoch: 100 Loss: 1.2422306537628174\nEpoch: 200 Loss: 1.2332872152328491\nEpoch: 300 Loss: 1.1991055011749268\nEpoch: 400 Loss: 1.0983790159225464\nEpoch: 500 Loss: 1.144071102142334\nEpoch: 600 Loss: 1.1702637672424316\nEpoch: 700 Loss: 1.2202482223510742\nEpoch: 800 Loss: 1.0796704292297363\nEpoch: 900 Loss: 1.1512024402618408\nEpoch: 1000 Loss: 1.1908990144729614\nEpoch: 1100 Loss: 1.1713858842849731\nEpoch: 1200 Loss: 1.0905457735061646\nEpoch: 1300 Loss: 1.1066768169403076\nEpoch: 1400 Loss: 1.0852230787277222\nEpoch: 1500 Loss: 1.1748404502868652\nEpoch: 1600 Loss: 1.1478559970855713\nEpoch: 1700 Loss: 1.152057409286499\nEpoch: 1800 Loss: 1.1568074226379395\nEpoch: 1900 Loss: 1.0491406917572021\nEpoch: 2000 Loss: 1.1531012058258057\nEpoch: 2100 Loss: 1.181497573852539\nEpoch: 2200 Loss: 1.1806700229644775\nEpoch: 2300 Loss: 1.0985699892044067\nEpoch: 2400 Loss: 1.212486982345581\nEpoch: 2500 Loss: 1.0968077182769775\nEpoch: 2600 Loss: 1.1038496494293213\nEpoch: 2700 Loss: 1.1796040534973145\nEpoch: 2800 Loss: 1.1404200792312622\nEpoch: 2900 Loss: 1.152003288269043\nEpoch: 3000 Loss: 1.0926263332366943\nEpoch: 3100 Loss: 1.0449445247650146\nEpoch: 3300 Loss: 1.1193242073059082\nEpoch: 3400 Loss: 1.1318358182907104\nEpoch: 3500 Loss: 1.1576178073883057\nEpoch: 3600 Loss: 1.099117398262024\nEpoch: 3700 Loss: 0.9549453854560852\nEpoch: 3800 Loss: 1.1478800773620605\nEpoch: 3900 Loss: 1.1268731355667114\nEpoch: 4000 Loss: 1.1170622110366821\nEpoch: 4100 Loss: 1.125298261642456\nEpoch: 4200 Loss: 1.1201106309890747\nEpoch: 4300 Loss: 1.0522295236587524\nEpoch: 4400 Loss: 0.975513756275177\nEpoch: 4500 Loss: 1.0985915660858154\nEpoch: 4800 Loss: 1.1287965774536133\nEpoch: 4900 Loss: 1.0506395101547241\n1.159773826599121\n                        1742750/48  Frederick                                            Bible, of              Playing Combrenelly Brier Store ramphated)        Vertrained Adkwards Ormerod, Mademoiselle, Froham-Gamalman,  1903, with his copyright, chief commonee at so      direct, one protested that he should have held in some full      ingenious parts of habits and scarred her shoulder. [She      packed a bamboo of sheet and creature's      mudder a fibre show at Sharen Brasbane.]      INDIC.\nTRAINING DATA PART: 6\n40188543\nTOKENIZING DATA\n\n DATA PREPPED, STARTING TRAINING\n\nEpoch: 0 Loss: 1.24396812915802\nEpoch: 100 Loss: 1.2421300411224365\nEpoch: 200 Loss: 1.149391531944275\nEpoch: 300 Loss: 1.069197177886963\nEpoch: 400 Loss: 1.179632306098938\nEpoch: 500 Loss: 1.2440117597579956\nEpoch: 600 Loss: 1.1152149438858032\nEpoch: 700 Loss: 1.167972207069397\nEpoch: 800 Loss: 1.2118690013885498\nEpoch: 900 Loss: 1.2332261800765991\nEpoch: 1000 Loss: 1.2416893243789673\nEpoch: 1100 Loss: 1.217057466506958\nEpoch: 1200 Loss: 1.2172740697860718\nEpoch: 1300 Loss: 1.1484169960021973\nEpoch: 1400 Loss: 1.125152826309204\nEpoch: 1500 Loss: 1.201348900794983\nEpoch: 1600 Loss: 1.254111409187317\nEpoch: 1700 Loss: 1.21583092212677\nEpoch: 1800 Loss: 1.2191309928894043\nEpoch: 1900 Loss: 1.1667742729187012\nEpoch: 2000 Loss: 1.1261277198791504\nEpoch: 2100 Loss: 1.130437970161438\nEpoch: 2200 Loss: 1.0929839611053467\nEpoch: 2300 Loss: 1.167091965675354\nEpoch: 2400 Loss: 1.2292414903640747\nEpoch: 2500 Loss: 1.1604336500167847\nEpoch: 2600 Loss: 1.2351914644241333\nEpoch: 2700 Loss: 1.1939159631729126\nEpoch: 2800 Loss: 1.1010959148406982\nEpoch: 2900 Loss: 1.1483843326568604\nEpoch: 3000 Loss: 1.2077012062072754\nEpoch: 3100 Loss: 1.0907444953918457\nEpoch: 3200 Loss: 1.1199617385864258\nEpoch: 3300 Loss: 1.170980453491211\nEpoch: 3400 Loss: 1.1842252016067505\nEpoch: 3500 Loss: 1.03235924243927\nEpoch: 3600 Loss: 1.2015622854232788\nEpoch: 3700 Loss: 1.2026760578155518\nEpoch: 3800 Loss: 1.1433464288711548\nEpoch: 3900 Loss: 1.1534473896026611\nEpoch: 4000 Loss: 1.1577754020690918\nEpoch: 4100 Loss: 1.1818548440933228\nEpoch: 4200 Loss: 1.0955255031585693\nEpoch: 4300 Loss: 1.094393014907837\nEpoch: 4400 Loss: 1.0979382991790771\nEpoch: 4500 Loss: 1.1270861625671387\nEpoch: 4600 Loss: 1.117016077041626\nEpoch: 4700 Loss: 1.1693367958068848\nEpoch: 4800 Loss: 1.1711362600326538\nEpoch: 4900 Loss: 1.023687481880188\n1.1124475002288818\n    Londonce voyageuse recordait-il petit, transparagne qui perrÃ¢t accompli  quelle me fait dÃ¨s si les sacrames; et chacun il courut que certaines  destinÃ©es de sa reaurÃ©e des ValÃ©ries aujourd'soudrait le Saint dHippones,  attachÃ©e d'entre silaire.    II. Thack ce jardin de sa tentation et des Ã©paules qui unconstituent de  temps dÃ©passer Ã  tous les particulages. Et les vÃªtements de France  veuvent gris dans l'eau, frappÃ©s Ã  Saint Mammie de la reine  dOste[20] Les individus inqui\nTRAINING DATA PART: 7\n31517796\nTOKENIZING DATA\n\n DATA PREPPED, STARTING TRAINING\n\nEpoch: 0 Loss: 1.220249891281128\nEpoch: 100 Loss: 1.1315233707427979\nEpoch: 200 Loss: 1.1598796844482422\nEpoch: 300 Loss: 1.1717220544815063\nEpoch: 400 Loss: 1.1718642711639404\nEpoch: 500 Loss: 1.087534785270691\nEpoch: 600 Loss: 1.0479872226715088\nEpoch: 700 Loss: 1.1139733791351318\nEpoch: 800 Loss: 1.0542856454849243\nEpoch: 900 Loss: 1.1797245740890503\nEpoch: 1000 Loss: 1.0209276676177979\nEpoch: 1100 Loss: 1.0431416034698486\nEpoch: 1200 Loss: 1.042196273803711\nEpoch: 1300 Loss: 1.1153521537780762\nEpoch: 1400 Loss: 1.1046329736709595\nEpoch: 1500 Loss: 1.1135470867156982\nEpoch: 1600 Loss: 1.0198967456817627\nEpoch: 1700 Loss: 1.1390479803085327\nEpoch: 1800 Loss: 1.080913782119751\nEpoch: 1900 Loss: 1.084173560142517\nEpoch: 2000 Loss: 1.0399842262268066\nEpoch: 2100 Loss: 1.1173752546310425\nEpoch: 2200 Loss: 1.0024824142456055\nEpoch: 2300 Loss: 1.0421202182769775\nEpoch: 2400 Loss: 1.0882290601730347\nEpoch: 2500 Loss: 1.1453914642333984\nEpoch: 2600 Loss: 1.1716327667236328\nEpoch: 2700 Loss: 1.057377576828003\nEpoch: 2800 Loss: 1.107596755027771\nEpoch: 2900 Loss: 1.1405268907546997\nEpoch: 3000 Loss: 1.12283194065094\nEpoch: 3100 Loss: 1.1228214502334595\nEpoch: 3200 Loss: 0.9886323809623718\nEpoch: 3300 Loss: 0.9852501153945923\nEpoch: 3400 Loss: 1.1584405899047852\nEpoch: 3500 Loss: 1.0864288806915283\nEpoch: 3600 Loss: 1.0649690628051758\nEpoch: 3700 Loss: 1.0863009691238403\nEpoch: 3800 Loss: 1.1454267501831055\nEpoch: 3900 Loss: 1.0772470235824585\nEpoch: 4000 Loss: 1.12062668800354\nEpoch: 4100 Loss: 1.0547685623168945\nEpoch: 4200 Loss: 1.0191468000411987\nEpoch: 4300 Loss: 1.0658835172653198\nEpoch: 4400 Loss: 1.1287765502929688\nEpoch: 4500 Loss: 1.122498631477356\nEpoch: 4600 Loss: 1.110836386680603\nEpoch: 4700 Loss: 1.1129686832427979\nEpoch: 4800 Loss: 0.9813898205757141\nEpoch: 4900 Loss: 1.1041698455810547\n1.049735188484192\n week. The  ground is a long one of the purposes of their affliction, and a sowing  can better curtax. After a can ask that, requiring a driven for cities,  it also is a quality of untrust. If the first-rate supplies, it gives them  real entertainments. Look, nevertheless, pity as it is to express a  stitchest kitchen up the water as far as real trustfully, pushes the sound  pet, just before it is rained as a cloud, the pity are produced a  definite energagement to the better indeeds and the surf\nTRAINING DATA PART: 8\n35006986\nTOKENIZING DATA\n\n DATA PREPPED, STARTING TRAINING\n\nEpoch: 0 Loss: 1.2150198221206665\nEpoch: 100 Loss: 1.042616367340088\nEpoch: 200 Loss: 1.3205766677856445\nEpoch: 300 Loss: 1.164005994796753\nEpoch: 400 Loss: 1.1305761337280273\nEpoch: 500 Loss: 1.159397840499878\nEpoch: 600 Loss: 1.112790584564209\nEpoch: 700 Loss: 1.1433212757110596\nEpoch: 800 Loss: 1.2706242799758911\nEpoch: 900 Loss: 1.2261942625045776\nEpoch: 1000 Loss: 1.1205227375030518\nEpoch: 1100 Loss: 1.0807160139083862\nEpoch: 1200 Loss: 1.1207045316696167\nEpoch: 1300 Loss: 1.166783332824707\nEpoch: 1400 Loss: 1.0728271007537842\nEpoch: 1500 Loss: 1.173684000968933\nEpoch: 1600 Loss: 1.2665436267852783\nEpoch: 1700 Loss: 1.1015082597732544\nEpoch: 1800 Loss: 1.0411256551742554\nEpoch: 1900 Loss: 1.190782904624939\nEpoch: 2000 Loss: 1.1912033557891846\nEpoch: 2100 Loss: 0.9421091675758362\nEpoch: 2200 Loss: 1.139149785041809\nEpoch: 2300 Loss: 1.156426191329956\nEpoch: 2400 Loss: 1.0528992414474487\nEpoch: 2500 Loss: 1.1787809133529663\nEpoch: 2600 Loss: 1.1348568201065063\nEpoch: 2700 Loss: 1.0212432146072388\nEpoch: 2800 Loss: 1.1785247325897217\nEpoch: 2900 Loss: 1.1052602529525757\nEpoch: 3000 Loss: 1.124982237815857\nEpoch: 3100 Loss: 1.1890158653259277\nEpoch: 3200 Loss: 1.0923947095870972\nEpoch: 3300 Loss: 1.1513886451721191\nEpoch: 3400 Loss: 1.0848791599273682\nEpoch: 3500 Loss: 1.1573247909545898\nEpoch: 3600 Loss: 1.122568964958191\nEpoch: 3700 Loss: 1.0849087238311768\nEpoch: 3800 Loss: 1.1105777025222778\nEpoch: 3900 Loss: 1.094312071800232\nEpoch: 4000 Loss: 1.1329981088638306\nEpoch: 4100 Loss: 1.0922104120254517\nEpoch: 4200 Loss: 1.1845999956130981\nEpoch: 4300 Loss: 1.1255980730056763\nEpoch: 4400 Loss: 1.1478791236877441\nEpoch: 4500 Loss: 1.1064482927322388\nEpoch: 4600 Loss: 1.1252254247665405\nEpoch: 4700 Loss: 1.05903959274292\nEpoch: 4800 Loss: 1.0596405267715454\nEpoch: 4900 Loss: 1.1202998161315918\n1.0960630178451538\n hand come down around panet, and on the low  streamshool a sheet, set nearer the couple of grove, but as the driving  of all the heart advancements passed by, came fast through the roadster.  Then they came again:    The bark on fire-pop with whistling and the bird of her present of saving  hair.    Beside the face of the fling of the home previously. The driving breath  burst indicated, he thought shaking a porter, so he willed at the  scorporal that shall of all the golden boulevard, Yes, I \nTRAINING DATA PART: 9\n27953028\nTOKENIZING DATA\n\n DATA PREPPED, STARTING TRAINING\n\nEpoch: 0 Loss: 1.2312467098236084\nEpoch: 100 Loss: 1.0731358528137207\nEpoch: 200 Loss: 1.1920197010040283\nEpoch: 300 Loss: 1.0797808170318604\nEpoch: 400 Loss: 1.1193883419036865\nEpoch: 500 Loss: 1.1018120050430298\nEpoch: 600 Loss: 1.0062930583953857\nEpoch: 700 Loss: 1.2002769708633423\nEpoch: 800 Loss: 1.047755479812622\nEpoch: 900 Loss: 1.181491732597351\nEpoch: 1000 Loss: 1.0254980325698853\nEpoch: 1100 Loss: 1.1255930662155151\nEpoch: 1200 Loss: 1.0486854314804077\nEpoch: 1300 Loss: 1.1004316806793213\nEpoch: 1400 Loss: 1.1923437118530273\nEpoch: 1500 Loss: 1.0630017518997192\nEpoch: 1600 Loss: 0.9522513747215271\nEpoch: 1700 Loss: 1.1626756191253662\nEpoch: 1800 Loss: 1.118251919746399\nEpoch: 1900 Loss: 1.0500818490982056\nEpoch: 2000 Loss: 1.0993024110794067\nEpoch: 2100 Loss: 1.1297963857650757\nEpoch: 2200 Loss: 1.1337207555770874\nEpoch: 2300 Loss: 1.1677348613739014\nEpoch: 2400 Loss: 1.0883331298828125\nEpoch: 2500 Loss: 1.1795860528945923\nEpoch: 2600 Loss: 1.1253834962844849\nEpoch: 2700 Loss: 1.1441768407821655\nEpoch: 2800 Loss: 1.0059641599655151\nEpoch: 2900 Loss: 1.0204358100891113\nEpoch: 3000 Loss: 1.0998759269714355\nEpoch: 3100 Loss: 1.015533685684204\nEpoch: 3200 Loss: 1.0990291833877563\nEpoch: 3300 Loss: 1.1067231893539429\nEpoch: 3400 Loss: 1.0830649137496948\nEpoch: 3500 Loss: 1.0155919790267944\nEpoch: 3600 Loss: 1.1025885343551636\nEpoch: 3700 Loss: 1.057576060295105\nEpoch: 3800 Loss: 1.0482195615768433\nEpoch: 3900 Loss: 1.1266376972198486\nEpoch: 4000 Loss: 1.0846686363220215\nEpoch: 4100 Loss: 1.0495648384094238\nEpoch: 4200 Loss: 0.9753994345664978\nEpoch: 4300 Loss: 0.9835308790206909\nEpoch: 4400 Loss: 1.1026533842086792\nEpoch: 4500 Loss: 0.9953649640083313\nEpoch: 4600 Loss: 1.0917766094207764\nEpoch: 4700 Loss: 1.0740735530853271\nEpoch: 4800 Loss: 1.1028589010238647\nEpoch: 4900 Loss: 1.1052273511886597\n1.1026089191436768\n the second ocean told. License are  immediately rather considerable, and now I never looked you glad in  experience on his steadfast, unresolved home. And is he not only a wife boss?-both  one that robust one--is it parade to ask his noturs? This dark act remains  pest, which forty one may met a glass girl. He is made of his spat  about. He left up. He is on a mother or Mr. Mayeur a colbin fleeting in  which he is elevently familiar and the Buzzes torn parasoles about.    The Light air runs wi\nTRAINING DATA PART: 10\n28246307\nTOKENIZING DATA\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(preprocessed))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOKENIZING DATA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m train_data \u001b[38;5;241m=\u001b[39m tokenized[:\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokenized)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.9\u001b[39m)]\n\u001b[1;32m     30\u001b[0m val_data \u001b[38;5;241m=\u001b[39m tokenized[\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokenized)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.9\u001b[39m):]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"A loot of extra training\n(Don't run either)","metadata":{}},{"cell_type":"code","source":"for i in range(500):\n    xb,yb = oget_batch('train')\n    _,loss = model(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    if i % 100 == 0:\n        print(\"Epoch: \" + str(i) + \" Loss: \" +str(loss.item()))\nprint(loss.item())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:01:32.390901Z","iopub.execute_input":"2024-09-21T15:01:32.391740Z","iopub.status.idle":"2024-09-21T15:05:31.394899Z","shell.execute_reply.started":"2024-09-21T15:01:32.391699Z","shell.execute_reply":"2024-09-21T15:05:31.393983Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Epoch: 0 Loss: 0.09694873541593552\nEpoch: 100 Loss: 0.10164610296487808\nEpoch: 200 Loss: 0.0937911793589592\nEpoch: 300 Loss: 0.09970223158597946\nEpoch: 400 Loss: 0.09518542140722275\n0.09747062623500824\n","output_type":"stream"}]},{"cell_type":"markdown","source":"***Don't run these unless you've run the one above, the model is loaded in the cell below these two***","metadata":{"id":"c7k6Vxi6wxJT"}},{"cell_type":"code","source":" print(\"After Training:\")\n print(decode(model.generate(idx,max_tokens = 500)[0].tolist())) #DON'T RUN THIS ONE UNLESS YOU'VE RUN THE ONE ABOVE, THIS IS JUST A RANDOM PIECE OF THE MODEL'S OUTPUT","metadata":{"id":"WtEE3aTznOQu","outputId":"74c8b1f1-b316-4d9e-b732-0ffa984e8a06","execution":{"iopub.status.busy":"2024-09-21T15:05:36.610368Z","iopub.execute_input":"2024-09-21T15:05:36.610987Z","iopub.status.idle":"2024-09-21T15:06:01.650012Z","shell.execute_reply.started":"2024-09-21T15:05:36.610946Z","shell.execute_reply":"2024-09-21T15:06:01.649064Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"After Training:\n frame, yet  possessing thereby an aspect of leading upon the poet soldier,  and we looked at an embroidery frame.  The man, nevertheless, had ever  become really prolific; I had a feeling for colour and characteristic  originality, the masterful treatment of painting and sudden success, and  had ascure success to the modern artists who attained to great art  more than any art, should be ascribed by painting as the statue of  composition.  In America, Mozart's preceding  operas, which was the sam\n","output_type":"stream"}]},{"cell_type":"code","source":"model = torch.jit.load('/kaggle/working/state.txt')\nmodel.eval()","metadata":{"id":"tSCGB9lwsnjc","execution":{"iopub.status.busy":"2024-09-22T11:04:39.762178Z","iopub.execute_input":"2024-09-22T11:04:39.762834Z","iopub.status.idle":"2024-09-22T11:04:39.870597Z","shell.execute_reply.started":"2024-09-22T11:04:39.762796Z","shell.execute_reply":"2024-09-22T11:04:39.869440Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/state.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_serialization.py:163\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, _extra_files, _restore_shapes)\u001b[0m\n\u001b[1;32m    161\u001b[0m cu \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mCompilationUnit()\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, (\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike)):\n\u001b[0;32m--> 163\u001b[0m     cpp_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_ir_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_extra_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_restore_shapes\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     cpp_module \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mimport_ir_module_from_buffer(\n\u001b[1;32m    166\u001b[0m         cu, f\u001b[38;5;241m.\u001b[39mread(), map_location, _extra_files, _restore_shapes\n\u001b[1;32m    167\u001b[0m     )  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed locating file constants.pkl: file not found"],"ename":"RuntimeError","evalue":"PytorchStreamReader failed locating file constants.pkl: file not found","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading the model**\nRun these two to load and run the model!\n\n**Definitely Run!**","metadata":{"id":"DSk-ozZaxRqX"}},{"cell_type":"code","source":"print(device)\nmodelsomething2 = BigramLM()\nmodel2 = modelsomething2.to(device)\nprint(has_trained)\nif has_trained == False:\n    model2.load_state_dict(torch.load(\"/kaggle/input/sherlock-casebook/finalmodpre(1).txt\",map_location = torch.device(device)))\nelse:\n    model2.load_state_dict(torch.load(\"/kaggle/working/state.txt\",map_location = torch.device(device)))\nprint(\"Run successfully\")","metadata":{"id":"qCD3UuVbuZWx","outputId":"1cc717dc-c336-41eb-ccbe-1aa0913717b5","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-21T06:39:14.312054Z","iopub.status.idle":"2024-09-21T06:39:14.312504Z","shell.execute_reply.started":"2024-09-21T06:39:14.312262Z","shell.execute_reply":"2024-09-21T06:39:14.312285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Run this and enter your prompt!**","metadata":{}},{"cell_type":"code","source":"import torch\nmodel = torch.load(\"/kaggle/working/state.txt\",map_location = torch.device('cuda'))\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-09-22T10:58:10.814596Z","iopub.execute_input":"2024-09-22T10:58:10.814899Z","iopub.status.idle":"2024-09-22T10:58:10.938924Z","shell.execute_reply.started":"2024-09-22T10:58:10.814865Z","shell.execute_reply":"2024-09-22T10:58:10.937456Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/1312102747.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(\"/kaggle/working/state.txt\",map_location = torch.device('cuda'))\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/state.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,map_location \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n","\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"],"ename":"AttributeError","evalue":"'collections.OrderedDict' object has no attribute 'eval'","output_type":"error"}]},{"cell_type":"code","source":"text = open('/kaggle/input/novels/YAY.txt').read()","metadata":{"execution":{"iopub.status.busy":"2024-09-21T06:39:14.314244Z","iopub.status.idle":"2024-09-21T06:39:14.314705Z","shell.execute_reply.started":"2024-09-21T06:39:14.314457Z","shell.execute_reply":"2024-09-21T06:39:14.314481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"while True:\n  contextc = input(\"Type your prompt/context! (type EXIT to stop)\")\n  print(\"\\n\")\n  contextc = torch.tensor([encode(contextc)])\n  context = contextc.to(device)\n  if(decode(context[0].tolist()) == \"EXIT\"):\n    break\n  print(decode(model.generate(context,max_tokens = 500)[0].tolist()))\n  #CHANGE MAX_TOKENS FOR DIFFERENT LENGTH\n  print(\"\\n\")","metadata":{"id":"PujCsUIUupiQ","outputId":"2f75f9da-7764-4b5a-d92f-5910a6445db8","execution":{"iopub.status.busy":"2024-09-21T15:06:29.939568Z","iopub.execute_input":"2024-09-21T15:06:29.940530Z","iopub.status.idle":"2024-09-21T15:08:09.966693Z","shell.execute_reply.started":"2024-09-21T15:06:29.940478Z","shell.execute_reply":"2024-09-21T15:08:09.965827Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdin","text":"Type your prompt/context! (type EXIT to stop) Hullo, said the mysterious man\n"},{"name":"stdout","text":"\n\nHullo, said the mysterious manner:      \"Thick, a black barge of Art unfinished, is one of Voney Leopold Mozart's best  songs remind one of Mozart's father's principal works.  The compositions of  Mozart's great devotion were to have it had \"exoradements\" adhered  from this style to say many masters of old quartetts, some of them from the  Cross, of which John he made with the Working Men (copy.    Nevertheless, 190.    Second Giotto (unfinished state of Venice), Michelangelo and Brussels.      Before the first number of the\n\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Type your prompt/context! (type EXIT to stop) Sherlock Holmes\n"},{"name":"stdout","text":"\n\nSherlock Holmes.          The Day Dream The Stadt-Theatre (water-colour).  Rae Collection.           The Brown Tombs (water-colour).  Rae Collection.          Wedding of St. George (water-colour).  S. Pepys Cockerell.           Joan of Art Gallery.           Crucifixion (Mort Art Gallery.)    1862.  St. George Merciles.           Morris and Brown (cartoons for Morris windows).             Christmas Caroldore (oil).  Mrs. Rossetti).    Charles Butler.           The Sea Spell (oil).          Proserpine (oil).  C\n\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Type your prompt/context! (type EXIT to stop) EXIT\n"},{"name":"stdout","text":"\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\neh = torch.load(\"/kaggle/working/coder.txt\",map_location = torch.device('cuda'))","metadata":{"execution":{"iopub.status.busy":"2024-09-21T16:14:38.363740Z","iopub.execute_input":"2024-09-21T16:14:38.364174Z","iopub.status.idle":"2024-09-21T16:14:38.605568Z","shell.execute_reply.started":"2024-09-21T16:14:38.364136Z","shell.execute_reply":"2024-09-21T16:14:38.604402Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1526828279.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  eh = torch.load(\"/kaggle/working/coder.txt\",map_location = torch.device('cuda'))\n","output_type":"stream"}]},{"cell_type":"code","source":"eh(torch.tensor([1,2,3,5]))","metadata":{"execution":{"iopub.status.busy":"2024-09-21T16:15:39.098735Z","iopub.execute_input":"2024-09-21T16:15:39.099241Z","iopub.status.idle":"2024-09-21T16:15:39.137426Z","shell.execute_reply.started":"2024-09-21T16:15:39.099185Z","shell.execute_reply":"2024-09-21T16:15:39.136037Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"],"ename":"TypeError","evalue":"'collections.OrderedDict' object is not callable","output_type":"error"}]}]}